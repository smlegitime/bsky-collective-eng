{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38ffd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def pretty_print(text, width=80):\n",
    "    wrapper = textwrap.TextWrapper(width=width, replace_whitespace=False)\n",
    "    # Handle both real \\n and escaped \\\\n\n",
    "    clean_text = text.replace(\"\\\\n\", \"\\n\").replace(\"\\u200b\", \"\")\n",
    "    paragraphs = clean_text.split(\"\\n\\n\")\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # Dedent so we don't get weird indents from source formatting\n",
    "        dedented = textwrap.dedent(para.strip())\n",
    "        print(wrapper.fill(dedented))\n",
    "        print()  # preserve blank line between paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38a4f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_SOURCES_PATH = '../constants/retrieval_sources.json'\n",
    "\n",
    "def _get_src_urls(src_path: str = RETRIEVAL_SOURCES_PATH):\n",
    "    with open(src_path) as retrieval_src_file:\n",
    "        retrieval_srcs = json.load(retrieval_src_file)\n",
    "        urls = retrieval_srcs['bsky'] + retrieval_srcs['skyware']\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67552826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AT Protocol | Bluesky\\n\\n\\n\\n\\n\\n\\nSkip to main contentBlueskyDocsBlogShowcaseGitHubSearchGet StartedTutorialsStarter TemplatesAdvanced GuidesThe AT ProtocolFederation ArchitectureLinks, mentions, and rich textRate LimitsLabels and moderationPostsTimestampsFirehoseResolving IdentitiesCustom SchemasBackfilling the NetworkRead-After-WriteService AuthPDS EntrywayoEmbed and Post Embed WidgetAction Intent LinksOAuth Client ImplementationAPI Hosts and AuthHTTP ReferenceSupportAdvanced GuidesThe AT ProtocolOn this pageThe AT Protocol\\nThe AT Protocol (Authenticated Transfer Protocol, or atproto) is a standard for public conversation and an open-source framework for building social apps.\\nIt creates a standard format for user identity, follows, and data on social apps, allowing apps to interoperate and users to move across them freely. It is a federated network with account portability.\\nBasic Concepts\\u200b\\nIdentity\\u200b\\nUsers are identified by domain names on the AT Protocol. These domains map to cryptogra'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = _get_src_urls()\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "docs[0][0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b232ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f50a44f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AT Protocol | Bluesky'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits[0].page_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e08babf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cafdd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_bsky_docs\",\n",
    "    \"Search and return information about the Bluesky social app and Bluesky labelers.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8dbc4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of the definitions you might use for a label\n",
      "\n",
      "Label configuration\n",
      "The user may choose to hide, warn, or ignore each label from\n",
      "a labeler. Hiding and warning are basically similar, except that hide will also\n",
      "filter the labeled content from feeds and listings. Ignore just ignores the\n",
      "label. If adult content is not enabled in preferences, the behavior should force\n",
      "to hide with no override.\n",
      "Labelers\n",
      "Labelers publish labels through a labeling\n",
      "service. They also receive reports through a reporting service.\n",
      "\n",
      "For this guide, we’ve set up an example labeler with four labels for a user to\n",
      "choose from: fire, water, air, and earth. These labels have been set to “inform”\n",
      "because they’re intended to be used for informational purposes, “warn” severity\n",
      "so that they appear on user profiles and posts, and no blur because these are\n",
      "not moderation labels.\n",
      "\n",
      "Developers building client applications should understand how to apply labels\n",
      "(#2) and user controls (#3). For more complete details, see the Labels\n",
      "Specification.\n",
      "Labels\n",
      "Labels are published by moderation services, which are\n",
      "either hardcoded into the application or chosen by the user. They are attached\n",
      "to records in the responses under the labels key.\n",
      "A label is published with the\n",
      "following information:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use retrieval tool defined above to search for information about \"label configuration\":\n",
    "# Returns amount of chunks specified in retriever.\n",
    "\n",
    "docs = retriever_tool.invoke({'query': 'label configuration'})\n",
    "pretty_print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9108e7",
   "metadata": {},
   "source": [
    "Evaluate doc relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "EVAL_PROMPT = (\n",
    "    \"You are an evaluator assessing the relevance of a retrieved document to a user question. \\n\"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "class EvaluateDocuments(BaseModel):\n",
    "    \"\"\"Evaluate documents using a binary score for relevance check\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "eval_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n",
    "\n",
    "def evaluate_documents(state: MessagesState) -> Literal['generate_answer', 'rewrite_question']:\n",
    "    question = state['messages'][0].content\n",
    "    context = state['messages'][-1].content\n",
    "\n",
    "    prompt = EVAL_PROMPT.format(context=context, question=question)\n",
    "\n",
    "    response = eval_model.with_structured_output(EvaluateDocuments).invoke(\n",
    "        [{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == 'yes':\n",
    "        return 'generate_answer'\n",
    "    else:\n",
    "        return 'rewrite_question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "55213666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What does the Bluesky documentation say about Personal Data Servers?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={}, response_metadata={}, tool_calls=[{'name': 'retrieve_bsky_docs', 'args': {'query': 'bluesky personal data server'}, 'id': '1', 'type': 'tool_call'}]), ToolMessage(content='sike!', tool_call_id='1')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:43:44 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does the Bluesky documentation say about Personal Data Servers?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_bsky_docs\",\n",
    "                        \"args\": {\"query\": \"bluesky personal data server\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"sike!\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "print(input)\n",
    "evaluate_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7e83a",
   "metadata": {},
   "source": [
    "Rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ca6f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n",
    "\n",
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a0cb9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What information does the Bluesky documentation provide regarding Personal Data Servers, including their purpose, functionality, and role within the Bluesky ecosystem?\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does the Bluesky documentation say about Personal Data Servers?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_bsky_docs\",\n",
    "                        \"args\": {\"query\": \"bluesky personal data server\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"sike!\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "# print(response)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b5f4777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What does the Bluesky documentation say about Personal Data Servers?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={}, response_metadata={}, tool_calls=[{'name': 'retrieve_bsky_docs', 'args': {'query': 'bluesky personal data server'}, 'id': '1', 'type': 'tool_call'}]), ToolMessage(content='sike!', tool_call_id='1')]}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "response = rewrite_question(input)\n",
    "# input[\"messages\"][0] = {\"role\": \"user\", \"content\": response} \n",
    "# print(type((input[\"messages\"][0])))\n",
    "print(type(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf85918",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks about Bluesky's moderation system. Use the following pieces of retrieved context to answer the question.\\n\"\n",
    "    \"If the question asks for label configuration, label definitions, or moderation settings, provide the appropriate configuration in the correct format (JSON, code snippets, or structured data as needed).\\n\"\n",
    "    \"Try your best to suggest label names, descriptions, and severity, based on the provided question and context.\\n\"\n",
    "    \"If the question asks for general information or explanations, use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question}\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c87d7d",
   "metadata": {},
   "source": [
    "** Notes on optimization testing: **\n",
    "\n",
    "The below script tests RAG performance w/ a single query while modifying the following: \n",
    "* Chunk size & chunk overlap \n",
    "   (less context loss but more noise & memory use:)\n",
    "    * big chunk, high overlap\n",
    "    * big chunk, low overlap\n",
    "   \n",
    "    (more specific but more context loss)\n",
    "    * small chunk, high overlap\n",
    "    * small chunk, low overlap\n",
    "\n",
    "We can repeat the above w/ different queries & diff models.\n",
    "\n",
    "Our default LLM has been OpenAI's, but it would be helpful for us to find the optimal chunking for an Ollama model, as inputted data then remains private and responses are not uncensored.\n",
    "\n",
    "Later exploration: diff chunking algorithms (what's the deal with late chunking?), different retrieval algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99683a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:33:50 - INFO - Hello world!\n"
     ]
    }
   ],
   "source": [
    "## testing setup: ##\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "logging.info('Hello world!')\n",
    "\n",
    "# sample input, copied from above w/ notes:\n",
    "input_state = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                # evaluate_documents expects question here:\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does the Bluesky documentation say about Personal Data Servers?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_bsky_docs\",\n",
    "                        \"args\": {\"query\": \"bluesky personal data server\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            # evaluate_documents expects context (i.e retrieved docs) here\n",
    "            # but why do we get a tool call instead?\n",
    "            {\"role\": \"tool\", \"content\": \"sike!\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edaff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (logic for this func is sound!)\n",
    "def build_retriever(CHUNK_SIZE=100, CHUNK_OVERLAP=50, embedder=\"openai\") -> VectorStoreRetriever:\n",
    "    # fetching & cleaning documentation:\n",
    "    urls = _get_src_urls()\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "    docs[0][0].page_content.strip()[:1000]\n",
    "\n",
    "    # chunking docs; storing docs as vector embeddings:\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    doc_splits[0].page_content.strip()\n",
    "\n",
    "    vectorstore = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=OpenAIEmbeddings()\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "def retrieve_docs(retriever, query=\"label configuration\") -> str:\n",
    "    \"\"\"Use a pre-defined retriever to build retrieval tool that queries database as prompted.\"\"\"\n",
    "    docs = retriever_tool.invoke({'query': query})\n",
    "    logging.info(f\"docs retrieved for query {query}: {pretty_print(docs)}\")\n",
    "    return docs\n",
    "\n",
    "# TODO: make sure the docs retrieved by retriever are passed in through state.\n",
    "def test_rag(state: \"MessagesState\"):\n",
    "    \"\"\"rewrite_question until evaluate_documents determines that retrieved docs are sufficiently relevant; then generate_answer.\"\"\"\n",
    "    # track all iterations of rewritten question & retrieved docs:\n",
    "    iterations = {}\n",
    "\n",
    "    while evaluate_documents(state) == 'rewrite_question':\n",
    "        response = rewrite_question(state)\n",
    "        iterations[\"rewritten question\"] = \"retrieved docs\"\n",
    "\n",
    "        # ^ TODO: Implement logic keeping track of (1) docs retrieved during this \n",
    "        # iteration & (2) rewritten question. Make sure new docs are retrieved & appended to state.\n",
    "\n",
    "        # NOTE for Sybille:\n",
    "        # rewrite_question outputs the new question as a dict, while evaluate_documents \n",
    "        # takes in a MessagesState object where item 0 is query to be evaluated, & item -1 \n",
    "        # is content (i.e. retrieved docs.). \n",
    "        # in this while loop, should new question replace old question, or be appended onto \n",
    "        # state alongside another tool call to our retriever?\n",
    "        # Unsure if the above should be done manually, w/ LangChain tools, or w/ an actual \n",
    "        # LangChain graph!\n",
    "\n",
    "    # return metadata after successful answer generation:\n",
    "    return {\n",
    "        \"state\": generate_answer(state),\n",
    "        \"iterations\": iterations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a54464b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_run(state: \"MessagesState\"):\n",
    "    output = []\n",
    "    for chunk_size in [200, 500, 1000]:\n",
    "        for overlap in [0, int(chunk_size*0.2), int(chunk_size*0.5)]:\n",
    "            retriever = build_retriever()\n",
    "            docs = retrieve_docs(retriever, \"label configuration\")\n",
    "            test_output = test_rag(state)\n",
    "            \n",
    "            output = {\n",
    "                 \"chunk_size\": chunk_size,\n",
    "                 \"chunk_overlap\": overlap,\n",
    "                 \"rewrite_retrieve_iterations\": test_output[\"iterations\"],\n",
    "                 \"final_state\": test_output[\"state\"]\n",
    "                 }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374675e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ignore - Ran is figuring out indexing)\n",
    "# # Rewrite question until evaluate_documents outputs \"generate answer\"\n",
    "# While evaluate_documents(input) == 'rewrite_question':\n",
    "    logging.info(\"evaluate_documents called.\")\n",
    "    response = rewrite_question(input)\n",
    "    input[\"messages\"][0] = {\"role\": \"user\", \"content\": response} \n",
    "    input_iterations += response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
